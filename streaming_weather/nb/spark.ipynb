{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb4a146-36ed-4609-a09e-96ce39692438",
   "metadata": {},
   "source": [
    "# Part 3: Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bfdd2f0-5099-4bd2-b4e1-04f4c6fc7090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-69cb86fe-edf7-4995-aa70-542ec8e0bc7a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.2/spark-sql-kafka-0-10_2.12-3.2.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2!spark-sql-kafka-0-10_2.12.jar (65ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.2.2/spark-token-provider-kafka-0-10_2.12-3.2.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2!spark-token-provider-kafka-0-10_2.12.jar (19ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (269ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (19ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.1/hadoop-client-runtime-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.1!hadoop-client-runtime.jar (542ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (21ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (36ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (19ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.1/hadoop-client-api-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.1!hadoop-client-api.jar (228ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.htrace#htrace-core4;4.1.0-incubating!htrace-core4.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (16ms)\n",
      ":: resolution report :: resolve 6554ms :: artifacts dl 1319ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   13  |   13  |   0   ||   13  |   13  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-69cb86fe-edf7-4995-aa70-542ec8e0bc7a\n",
      "\tconfs: [default]\n",
      "\t13 artifacts copied, 0 already retrieved (59188kB/105ms)\n",
      "23/04/28 22:14:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .config(\"spark.ui.showConsoleProgress\", False)\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2')\n",
    "         .getOrCreate())\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"stations-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62beecb7-7ddb-4bf9-85bc-a3a1c8971514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"station\", StringType()),\n",
    "    StructField(\"date\", DateType()),\n",
    "    StructField(\"degrees\", DoubleType()),\n",
    "    StructField(\"raining\", IntegerType())\n",
    "])\n",
    "\n",
    "reports = (df.select(col(\"key\").cast(\"string\"),\n",
    "          from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    " .select(\"key\", \"value.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e3ebe8-cb05-4aa9-b0a6-a280737762fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 22:14:36 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-57c89ebd-d15c-4e3e-a666-ee1a835d5bab. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/28 22:14:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|     start|       end|measurements|               avg|              max|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|      A|2000-01-01|2000-05-11|         132| 44.37531906496356|83.14623638884372|\n",
      "|      B|2000-01-01|2000-05-11|         132| 41.82788631855551|77.93431499850729|\n",
      "|      C|2000-01-01|2000-05-11|         132| 42.13010361404884| 69.7491512914404|\n",
      "|      D|2000-01-01|2000-05-11|         132|   39.349177322068|77.82482218878721|\n",
      "|      E|2000-01-01|2000-05-11|         132| 51.06049152420281|98.05591787335769|\n",
      "|      F|2000-01-01|2000-05-11|         132|  49.3181409588246|84.62408916989543|\n",
      "|      G|2000-01-01|2000-05-11|         132| 39.15908788362907|75.99755768600086|\n",
      "|      H|2000-01-01|2000-05-11|         132|38.142825655778246|83.25866153092528|\n",
      "|      I|2000-01-01|2000-05-11|         132| 29.58600197048384|63.69652698771509|\n",
      "|      J|2000-01-01|2000-05-11|         132| 40.65706222568739| 83.0015093426894|\n",
      "|      K|2000-01-01|2000-05-11|         132|24.918495370024182|58.84000926647204|\n",
      "|      L|2000-01-01|2000-05-11|         132| 28.75264523474327|60.43446823449474|\n",
      "|      M|2000-01-01|2000-05-11|         132|  40.8802386661891|68.54305071210767|\n",
      "|      N|2000-01-01|2000-05-11|         132| 55.69634939886712|81.43636211907075|\n",
      "|      O|2000-01-01|2000-05-11|         132| 42.12618357210941|86.73980545209731|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 22:14:53 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 16915 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|     start|       end|measurements|               avg|              max|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|      A|2000-01-01|2000-05-22|         143|46.750392265033014|85.93474454206387|\n",
      "|      B|2000-01-01|2000-05-22|         143| 44.16263853766018|79.53021122933745|\n",
      "|      C|2000-01-01|2000-05-22|         143| 43.57648005980836|70.84355761504722|\n",
      "|      D|2000-01-01|2000-05-22|         143| 42.00611481234511|84.58195456530389|\n",
      "|      E|2000-01-01|2000-05-22|         143|53.738681913803006|98.05591787335769|\n",
      "|      F|2000-01-01|2000-05-22|         143| 51.32604380969941|84.62408916989543|\n",
      "|      G|2000-01-01|2000-05-22|         143|  41.4940250697281|76.50130524519331|\n",
      "|      H|2000-01-01|2000-05-22|         143| 39.89146115520232|83.25866153092528|\n",
      "|      I|2000-01-01|2000-05-22|         143| 32.00025489150373|68.70631341222673|\n",
      "|      J|2000-01-01|2000-05-22|         143| 42.86049750316877| 83.0015093426894|\n",
      "|      K|2000-01-01|2000-05-22|         143|28.673594161296588|85.34083446887277|\n",
      "|      L|2000-01-01|2000-05-22|         143|30.336616984170984|60.43446823449474|\n",
      "|      M|2000-01-01|2000-05-22|         143|41.888101796782834|68.54305071210767|\n",
      "|      N|2000-01-01|2000-05-22|         143| 56.50371723246612|81.43636211907075|\n",
      "|      O|2000-01-01|2000-05-22|         143| 44.70803288697432|86.73980545209731|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|     start|       end|measurements|               avg|              max|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|      A|2000-01-01|2000-05-26|         147| 47.15859641739498|85.93474454206387|\n",
      "|      B|2000-01-01|2000-05-26|         147| 44.86459862166865|79.53021122933745|\n",
      "|      C|2000-01-01|2000-05-26|         147|44.131246422395634|70.84355761504722|\n",
      "|      D|2000-01-01|2000-05-26|         147| 43.05294702883296|86.35294928799097|\n",
      "|      E|2000-01-01|2000-05-26|         147| 54.38747956645826|98.05591787335769|\n",
      "|      F|2000-01-01|2000-05-26|         147| 52.01340161751495|84.62408916989543|\n",
      "|      G|2000-01-01|2000-05-26|         147| 42.45544024194993| 83.7623038605472|\n",
      "|      H|2000-01-01|2000-05-26|         147|40.396721939593554|83.25866153092528|\n",
      "|      I|2000-01-01|2000-05-26|         147| 32.69477993436962|68.70631341222673|\n",
      "|      J|2000-01-01|2000-05-26|         147| 43.57130972830459| 83.0015093426894|\n",
      "|      K|2000-01-01|2000-05-26|         147| 29.57198911317565|85.34083446887277|\n",
      "|      L|2000-01-01|2000-05-26|         147| 30.67450292467435|60.43446823449474|\n",
      "|      M|2000-01-01|2000-05-26|         147|  42.5505264750336| 70.1243398573786|\n",
      "|      N|2000-01-01|2000-05-26|         147|56.973781044337635|81.43636211907075|\n",
      "|      O|2000-01-01|2000-05-26|         147| 44.95721235698377|86.73980545209731|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|     start|       end|measurements|               avg|              max|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|      A|2000-01-01|2000-05-30|         151| 48.00519754049804|86.99125212473308|\n",
      "|      B|2000-01-01|2000-05-30|         151|45.260190367413074|79.53021122933745|\n",
      "|      C|2000-01-01|2000-05-30|         151|  44.5693054165077|70.84355761504722|\n",
      "|      D|2000-01-01|2000-05-30|         151| 43.81648662714295|86.35294928799097|\n",
      "|      E|2000-01-01|2000-05-30|         151|55.087956357505824|98.05591787335769|\n",
      "|      F|2000-01-01|2000-05-30|         151|52.631029464640115|84.62408916989543|\n",
      "|      G|2000-01-01|2000-05-30|         151|   42.940823433709| 83.7623038605472|\n",
      "|      H|2000-01-01|2000-05-30|         151|41.241513861545435|83.25866153092528|\n",
      "|      I|2000-01-01|2000-05-30|         151|33.217235509311344|68.70631341222673|\n",
      "|      J|2000-01-01|2000-05-30|         151| 44.08324466610799| 83.0015093426894|\n",
      "|      K|2000-01-01|2000-05-30|         151| 30.30676569917419|85.34083446887277|\n",
      "|      L|2000-01-01|2000-05-30|         151|30.983502521759615|60.43446823449474|\n",
      "|      M|2000-01-01|2000-05-30|         151|43.100973358716764|74.85996190754862|\n",
      "|      N|2000-01-01|2000-05-30|         151|57.346983392464864|81.43636211907075|\n",
      "|      O|2000-01-01|2000-05-30|         151| 45.37844721162284|86.73980545209731|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 22:15:07 WARN TaskSetManager: Lost task 4.0 in stage 20.0 (TID 148) (9b4885b177fc executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/28 22:15:07 WARN TaskSetManager: Lost task 5.0 in stage 20.0 (TID 149) (9b4885b177fc executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, min, max, count, first, last, asc\n",
    "\n",
    "counts_df = (reports.groupBy(col(\"station\")).agg(\n",
    "    min(col(\"date\")).alias(\"start\"),\n",
    "    max(col(\"date\")).alias(\"end\"),\n",
    "    count(col(\"degrees\")).alias(\"measurements\"),\n",
    "    avg(col(\"degrees\")).alias(\"avg\"),\n",
    "    max(col(\"degrees\")).alias(\"max\"))\n",
    "             .orderBy(asc(\"station\"))\n",
    ")\n",
    "\n",
    "s = counts_df.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"complete\").start()\n",
    "s.awaitTermination(30)\n",
    "s.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a4364c-37c4-440e-9278-1da2da1ed223",
   "metadata": {},
   "source": [
    "## Rain Forecast Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6b9f2a-31ff-46b6-8d1d-1367a507184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = reports.select(col(\"station\"), col(\"date\"), col(\"raining\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97a89f1-2625-49bb-a17d-b9dba8659115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "yesterday = (reports.withColumn(\"date\", date_add(\"date\", 1))\n",
    "             .select(col(\"station\"), col(\"date\"), month(col(\"date\")).alias(\"month\"), \n",
    "                     col(\"degrees\").alias(\"sub1degrees\"), col(\"raining\").alias(\"sub1raining\")))\n",
    "\n",
    "two_days_ago = (reports.withColumn(\"date\", date_add(\"date\", 2))\n",
    "             .select(col(\"station\"), col(\"date\"), month(col(\"date\")).alias(\"month\"), \n",
    "                     col(\"degrees\").alias(\"sub2degrees\"), col(\"raining\").alias(\"sub2raining\")))\n",
    "\n",
    "features = yesterday.join(two_days_ago, on = [\"station\", \"date\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5022e054-ca63-4392-a6fc-8e53eeffab38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 22:15:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "today_features = today.join(features, on = [\"date\", \"station\"])\n",
    "\n",
    "import os\n",
    "if \"reports\" in os.listdir() and \"checkpoint\" in os.listdir():\n",
    "    ! rm -r reports\n",
    "    ! rm -r checkpoint\n",
    "\n",
    "stream = (today_features.repartition(1)\n",
    "          .writeStream\n",
    "          .format(\"parquet\")\n",
    "          .option(\"path\", \"reports\")\n",
    "          .option(\"checkpointLocation\", \"checkpoint\")\n",
    "          .trigger(processingTime = \"1 minute\")\n",
    "          .start()\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c5f6e-2ba5-4317-a690-2b2c23a58383",
   "metadata": {},
   "source": [
    "# Part 4: Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e570a0f-3ad4-4aa6-a129-ad98d2edfdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85a48ee-ae81-452a-94dc-7c58e5a8b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[\"month\", \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\"], outputCol=\"features\")\n",
    "data = va.transform(spark.read.parquet(\"reports\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246ff1ba-0181-4992-833d-25b6fece2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a47ad85-d1e8-4295-9ae8-238a7095f0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_d8273381379e, depth=5, numNodes=21, numClasses=2, numFeatures=5\n",
      "  If (feature 2 <= 0.5)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 0.5)\n",
      "   If (feature 1 <= 39.65653895139251)\n",
      "    If (feature 0 <= 3.5)\n",
      "     If (feature 1 <= 35.37284361547289)\n",
      "      Predict: 0.0\n",
      "     Else (feature 1 > 35.37284361547289)\n",
      "      If (feature 1 <= 37.631436006253544)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1 > 37.631436006253544)\n",
      "       Predict: 0.0\n",
      "    Else (feature 0 > 3.5)\n",
      "     If (feature 3 <= 41.764969818280626)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 41.764969818280626)\n",
      "      If (feature 3 <= 44.136453106919475)\n",
      "       Predict: 0.0\n",
      "      Else (feature 3 > 44.136453106919475)\n",
      "       Predict: 1.0\n",
      "   Else (feature 1 > 39.65653895139251)\n",
      "    If (feature 1 <= 81.25537642647976)\n",
      "     If (feature 3 <= 46.37543614039134)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 46.37543614039134)\n",
      "      If (feature 0 <= 2.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 0 > 2.5)\n",
      "       Predict: 1.0\n",
      "    Else (feature 1 > 81.25537642647976)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(featuresCol = \"features\", labelCol = \"raining\")\n",
    "dt_model = dt_classifier.fit(train_data)\n",
    "print(dt_model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18cb15f1-530e-46cf-90f6-0ef2a332dc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|       avg_correct|        avg_raining|\n",
      "+------------------+-------------------+\n",
      "|0.8185745140388769|0.30309575233981284|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol = \"prediction\", \n",
    "                                              labelCol = \"raining\", metricName = \"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "avg_raining = test_data.filter(col(\"raining\") == 1).count() / test_data.count()\n",
    "\n",
    "predict_schema = StructType([\n",
    "    StructField(\"avg_correct\", DoubleType()),\n",
    "    StructField(\"avg_raining\", DoubleType()),\n",
    "])\n",
    "\n",
    "predict_analysis = spark.createDataFrame([(accuracy, avg_raining)], schema = predict_schema)\n",
    "predict_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bcf562-825a-4d86-b044-3aabcbbe4e7a",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b734b48b-8bf7-48d0-a691-556ed230b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f394317-d6ad-4d53-8b08-fd2e1f0d8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "va_model = VectorAssembler(inputCols=[\"month\", \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\"], \n",
    "                           outputCol=\"predict_features\")\n",
    "dt_classifier = DecisionTreeClassifier(featuresCol = \"predict_features\", labelCol = \"raining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe4a7d59-d428-4d90-bede-8987a9393d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 22:21:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a68f67e4-5871-4fb3-ad62-74e00866d07d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/28 22:21:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2000-02-15|       0.0|\n",
      "|      A|2000-02-17|       0.0|\n",
      "|      A|2000-02-21|       0.0|\n",
      "|      A|2000-02-24|       0.0|\n",
      "|      A|2000-03-15|       1.0|\n",
      "|      A|2000-04-01|       0.0|\n",
      "|      A|2000-04-02|       0.0|\n",
      "|      A|2000-04-14|       1.0|\n",
      "|      A|2000-04-19|       1.0|\n",
      "|      A|2000-05-23|       1.0|\n",
      "|      A|2000-05-24|       1.0|\n",
      "|      A|2000-06-14|       0.0|\n",
      "|      A|2000-06-15|       0.0|\n",
      "|      A|2000-06-26|       1.0|\n",
      "|      A|2000-07-11|       0.0|\n",
      "|      A|2000-07-27|       1.0|\n",
      "|      A|2000-08-07|       0.0|\n",
      "|      A|2000-08-09|       0.0|\n",
      "|      A|2000-08-16|       0.0|\n",
      "|      A|2000-09-03|       1.0|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2001-06-09|       0.0|\n",
      "|      A|2001-06-06|       0.0|\n",
      "|      A|2001-06-07|       1.0|\n",
      "|      A|2001-06-10|       0.0|\n",
      "|      A|2001-06-11|       0.0|\n",
      "|      A|2001-06-08|       0.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2001-06-13|       1.0|\n",
      "|      A|2001-06-14|       1.0|\n",
      "|      A|2001-06-16|       0.0|\n",
      "|      A|2001-06-15|       1.0|\n",
      "|      A|2001-06-12|       1.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2001-06-18|       0.0|\n",
      "|      A|2001-06-17|       0.0|\n",
      "|      A|2001-06-20|       0.0|\n",
      "|      A|2001-06-19|       0.0|\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(stages=[\n",
    "    va_model,\n",
    "    dt_classifier\n",
    "])\n",
    "\n",
    "pipe_model = pipe.fit(train_data)\n",
    "\n",
    "predictions = pipe_model.transform(features)\n",
    "\n",
    "last_part = (predictions.filter(col(\"station\") == \"A\")\n",
    "     .select(col(\"station\"), col(\"date\"), col(\"prediction\"))\n",
    "     .writeStream\n",
    "     .outputMode(\"append\")\n",
    "     .format(\"console\")\n",
    "     .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b16a9905-5b65-484c-9464-a1bb02e7529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 22:21:36 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@30d2bd61 is aborting.\n",
      "23/04/28 22:21:36 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@30d2bd61 aborted.\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/09/temp_shuffle_d51569e6-b846-4701-8ee9-46774a464b38, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/22/temp_shuffle_b4f8ca16-e046-4081-9585-00e482b9094f, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/03/temp_shuffle_c3bea394-d252-458b-8fbe-9b2a9e1a506d, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/07/temp_shuffle_3cedae25-aa1f-4d01-9e62-58da12b81bb8, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/03/temp_shuffle_79c1e0ee-41fe-4b9c-8511-b78b9402a469, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/2a/temp_shuffle_594980a0-2a2b-430f-b85d-71e870a15dcb, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/34/temp_shuffle_b3773f20-556e-41fc-ab95-b8dfd08bf22c, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/0c/temp_shuffle_67ae095d-17de-42fe-8487-9d13c5e8d9fb, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/24/temp_shuffle_1a03fcfa-e658-4296-aaa5-025ee079096c, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/27/temp_shuffle_ad7b756f-ada4-4478-bb5e-0b0bac07e238, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/2a/temp_shuffle_125a0f3f-59b4-491e-852c-ad71e0c6c77b, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/12/temp_shuffle_55777784-38b9-4438-9028-d363fa2d8602, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/23/temp_shuffle_54cef0ca-dd73-4588-9638-e560499dab6a, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/1b/temp_shuffle_1ef4a592-1596-4ded-a1c0-8179cc2a1759, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/2c/temp_shuffle_1a5e2bd3-4445-4534-bb9c-09540ab4f7a2, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/30/temp_shuffle_3096a9a8-b720-4762-b583-c774baa85d79, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/1d/temp_shuffle_a2f9affd-dafc-4faf-bde7-ce00d5de873b, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/22/temp_shuffle_5d5b59ab-fc1e-4f74-b6b0-f7363da8afb1, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/05/temp_shuffle_d86b8241-f541-4111-abe1-2c222f5ebe34, null\n",
      "23/04/28 22:21:36 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-75436352-b253-4cb7-a721-dc1cc0ab9264/21/temp_shuffle_4ad3c68b-cb13-4bb9-9ccb-d5555494c2bd, null\n",
      "23/04/28 22:21:36 WARN TaskSetManager: Lost task 3.0 in stage 122.0 (TID 593) (9b4885b177fc executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/28 22:21:36 WARN TaskSetManager: Lost task 2.0 in stage 122.0 (TID 592) (9b4885b177fc executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "last_part.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df63bebf-e534-4b00-a29d-f68c5d18658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
