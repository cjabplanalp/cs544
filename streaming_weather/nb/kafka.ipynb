{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f060b190-483f-43b1-98dd-3472e5a4a8a7",
   "metadata": {},
   "source": [
    "# Part 1: Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf4d63a-f8ef-4c80-b54d-a2b1d2242492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time, random, string\n",
    "\n",
    "def one_station(name):\n",
    "    # temp pattern\n",
    "    month_avg = [27,31,44,58,70,79,83,81,74,61,46,32]\n",
    "    shift = (random.random()-0.5) * 30\n",
    "    month_avg = [m + shift + (random.random()-0.5) * 5 for m in month_avg]\n",
    "    \n",
    "    # rain pattern\n",
    "    start_rain = [0.1,0.1,0.3,0.5,0.4,0.2,0.2,0.1,0.2,0.2,0.2,0.1]\n",
    "    shift = (random.random()-0.5) * 0.1\n",
    "    start_rain = [r + shift + (random.random() - 0.5) * 0.2 for r in start_rain]\n",
    "    stop_rain = 0.2 + random.random() * 0.2\n",
    "\n",
    "    # day's state\n",
    "    today = datetime.date(2000, 1, 1)\n",
    "    temp = month_avg[0]\n",
    "    raining = False\n",
    "    \n",
    "    # gen weather\n",
    "    while True:\n",
    "        # choose temp+rain\n",
    "        month = today.month - 1\n",
    "        temp = temp * 0.8 + month_avg[month] * 0.2 + (random.random()-0.5) * 20\n",
    "        if temp < 32:\n",
    "            raining=False\n",
    "        elif raining and random.random() < stop_rain:\n",
    "            raining = False\n",
    "        elif not raining and random.random() < start_rain[month]:\n",
    "            raining = True\n",
    "\n",
    "        yield (today.strftime(\"%Y-%m-%d\"), name, temp, raining)\n",
    "\n",
    "        # next day\n",
    "        today += datetime.timedelta(days=1)\n",
    "        \n",
    "def all_stations(count=10, sleep_sec=1):\n",
    "    assert count <= 26\n",
    "    stations = []\n",
    "    for name in string.ascii_uppercase[:count]:\n",
    "        stations.append(one_station(name))\n",
    "    while True:\n",
    "        for station in stations:\n",
    "            yield next(station)\n",
    "        time.sleep(sleep_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40472cf-838c-4014-bbaf-4d4ffadaec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2000-01-01', 'A', 23.106934846763195, False)\n",
      "('2000-01-01', 'B', 26.868381336544704, False)\n",
      "('2000-01-01', 'C', 30.00862060013644, False)\n",
      "('2000-01-02', 'A', 22.701053120504632, False)\n",
      "('2000-01-02', 'B', 18.15590022054152, False)\n",
      "('2000-01-02', 'C', 29.33548732825825, False)\n",
      "('2000-01-03', 'A', 28.510150537314907, False)\n",
      "('2000-01-03', 'B', 12.677082076290707, False)\n",
      "('2000-01-03', 'C', 41.19896433804881, False)\n",
      "('2000-01-04', 'A', 31.41734889560798, False)\n"
     ]
    }
   ],
   "source": [
    "# loops forever because the weather never ends...\n",
    "count = 0\n",
    "for row in all_stations(3):\n",
    "    count += 1\n",
    "    print(row) # date, station, temp, raining\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5753747b-e170-4735-bbfc-cd1e44bc415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot delete (may not exist yet)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stations-json', 'stations']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer, TopicPartition\n",
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers=[\"kafka:9092\"])\n",
    "try:\n",
    "    admin.delete_topics([\"stations\", \"stations-json\"])\n",
    "    print(\"deleted\")\n",
    "except UnknownTopicOrPartitionError:\n",
    "    print(\"cannot delete (may not exist yet)\")\n",
    "\n",
    "time.sleep(1)\n",
    "admin.create_topics([NewTopic(\"stations\", 6, 1)])\n",
    "admin.create_topics([NewTopic(\"stations-json\", 6, 1)])\n",
    "admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0606ef65-7412-4bd6-8d59-bb963459a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"report_pb2.py\" in os.listdir():\n",
    "    ! rm report_pb2.py\n",
    "! python3 -m grpc_tools.protoc -I=. --python_out=. report.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bfc684f-c0cf-42c5-9303-8aeb49664bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from report_pb2 import *\n",
    "import json, threading\n",
    "\n",
    "def produce():\n",
    "    producer = KafkaProducer(bootstrap_servers=[\"kafka:9092\"], acks=\"all\", retries=10)\n",
    "    \n",
    "    for date, station, degrees, raining in all_stations(15):\n",
    "        # send to \"stations\" stream using protobuf\n",
    "        stations_proto = Report(date = date, station = station, degrees = degrees, raining = raining)\n",
    "        proto_value = stations_proto.SerializeToString()\n",
    "        producer.send(\"stations\", value = proto_value, key = bytes(station, \"utf-8\")) \n",
    "        \n",
    "        # send to \"stations-json\" using JSON\n",
    "        raining_int = int(raining)\n",
    "        stations_json = {\"date\":date, \"station\":station, \"degrees\":degrees, \"raining\":raining_int}\n",
    "        json_value = bytes(json.dumps(stations_json), \"utf-8\")\n",
    "        producer.send(\"stations-json\", value = json_value, key = bytes(station, \"utf-8\"))\n",
    "\n",
    "# start thread to run produce\n",
    "# never join thread because we want it to run forever\n",
    "threading.Thread(target=produce).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78787366-5245-486c-94fa-ae4f953d2564",
   "metadata": {},
   "source": [
    "# Part 2: Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "311bd044-968f-47fb-b2ee-b28c34016ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in range(6):\n",
    "    path = f\"partition-{partition}.json\"\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29586fc6-6dd4-410a-95bd-eadc7291adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partition(partition_num):\n",
    "    path = f\"partition-{partition_num}.json\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return {\"partition\":partition_num, \"offset\":0}\n",
    "\n",
    "def save_partition(partition):\n",
    "    path = f\"partition-{partition['partition']}.json\"\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(partition, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c074938-a7cd-4965-b970-be5b23831d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def station_update(prev_json, messages):\n",
    "    for msg in messages:\n",
    "        report = Report.FromString(msg.value)\n",
    "        if report.station in prev_json.keys():\n",
    "            # checking if message date is before end date in json file, using datetime to evaluate\n",
    "            curr_end = datetime.datetime.strptime(prev_json[report.station][\"end\"], '%Y-%m-%d')\n",
    "            new_date = datetime.datetime.strptime(report.date, '%Y-%m-%d')\n",
    "            \n",
    "            # if it's already been seen, next iteration\n",
    "            if new_date <= curr_end:\n",
    "                continue\n",
    "                \n",
    "            # else, add new date's stats and update for most recent data\n",
    "            prev_json[report.station][\"sum\"] += report.degrees\n",
    "            prev_json[report.station][\"count\"] += 1\n",
    "            prev_json[report.station][\"avg\"] = prev_json[report.station][\"sum\"] / prev_json[report.station][\"count\"]\n",
    "            \n",
    "            # convert date strings to time, use datetime to evaluate\n",
    "            curr_start = datetime.datetime.strptime(prev_json[report.station][\"start\"], '%Y-%m-%d')\n",
    "            \n",
    "            if new_date > curr_end:\n",
    "                prev_json[report.station][\"end\"] = report.date\n",
    "            elif new_date < curr_start:\n",
    "                prev_json[report.station][\"start\"] = report.date\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            # station not in json, update with single current info\n",
    "            # (to be updated later)\n",
    "            prev_json[report.station] = {}\n",
    "            prev_json[report.station][\"sum\"] = report.degrees\n",
    "            prev_json[report.station][\"count\"] = 1\n",
    "            prev_json[report.station][\"avg\"] = prev_json[report.station][\"sum\"] / prev_json[report.station][\"count\"]\n",
    "            \n",
    "            # setting start and end date to be the first date received\n",
    "            prev_json[report.station][\"start\"] = report.date\n",
    "            prev_json[report.station][\"end\"] = report.date\n",
    "            \n",
    "    return prev_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86917b4b-dcef-44db-9eea-873277acb260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 0\n",
      "exiting\n",
      "exiting\n",
      "exiting\n",
      "ROUND 1\n",
      "exiting\n",
      "exiting\n",
      "exiting\n"
     ]
    }
   ],
   "source": [
    "def consume(part_nums=[], iterations=10):\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[\"kafka:9092\"])\n",
    "    # create list of TopicPartition objects\n",
    "    consumer.assign([TopicPartition(\"stations\", part_num) for part_num in part_nums])\n",
    "\n",
    "    # PART 1: initialization\n",
    "    partitions = {} # key=partition num, value=snapshot dict\n",
    "    \n",
    "    for part_num in part_nums:\n",
    "         # load partitions from JSON files (if they exist) or create fresh dicts\n",
    "        partition_data = load_partition(part_num)\n",
    "        partitions[part_num] = partition_data\n",
    "        # if offsets were specified in previous JSON files, the consumer\n",
    "        # should seek to those; else, seek to offset 0 (in load_partition).\n",
    "        offset = partition_data[\"offset\"]\n",
    "        consumer.seek(TopicPartition(\"stations\", part_num), offset)\n",
    "    \n",
    "    # PART 2: process batches\n",
    "    for i in range(iterations):\n",
    "        batch = consumer.poll(1000) # 1s timeout\n",
    "        for topic, messages in batch.items():\n",
    "            partition_dict = partitions[topic.partition]\n",
    "            \n",
    "            # update the partitions based on new messages\n",
    "            updated_json = station_update(partition_dict, messages)\n",
    "            updated_json[\"offset\"] = consumer.position(topic)\n",
    "            \n",
    "            # save the data back to the JSON file\n",
    "            save_partition(updated_json)\n",
    "    print(\"exiting\")\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"ROUND\", i)\n",
    "    t1 = threading.Thread(target=consume, args=([0,1], 30))\n",
    "    t2 = threading.Thread(target=consume, args=([2,3], 30))\n",
    "    t3 = threading.Thread(target=consume, args=([4,5], 30))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3939eded-7a5d-44da-88a5-44908ac40b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"partition\": 0, \"offset\": 41, \"N\": {\"sum\": 1979.369445279752, \"count\": 41, \"avg\": 48.277303543408586, \"start\": \"2000-01-01\", \"end\": \"2000-02-10\"}}{\"partition\": 1, \"offset\": 82, \"E\": {\"sum\": 1380.0503764560815, \"count\": 41, \"avg\": 33.659765279416625, \"start\": \"2000-01-01\", \"end\": \"2000-02-10\"}, \"O\": {\"sum\": 1182.5191599373866, \"count\": 41, \"avg\": 28.84193073018016, \"start\": \"2000-01-01\", \"end\": \"2000-02-10\"}}{\"partition\": 2, \"offset\": 126, \"F\": {\"sum\": 1588.2846551420698, \"count\": 42, \"avg\": 37.816301312906425, \"start\": \"2000-01-01\", \"end\": \"2000-02-11\"}, \"I\": {\"sum\": 788.9024095308246, \"count\": 42, \"avg\": 18.783390703114872, \"start\": \"2000-01-01\", \"end\": \"2000-02-11\"}, \"J\": {\"sum\": 966.4667346622, \"count\": 42, \"avg\": 23.011112730052382, \"start\": \"2000-01-01\", \"end\": \"2000-02-11\"}}{\"partition\": 3, \"offset\": 126, \"D\": {\"sum\": 947.1333179027599, \"count\": 42, \"avg\": 22.550793283399045, \"start\": \"2000-01-01\", \"end\": \"2000-02-11\"}, \"G\": {\"sum\": 855.1238244642853, \"count\": 42, \"avg\": 20.36009105867346, \"start\": \"2000-01-01\", \"end\": \"2000-02-11\"}, \"M\": {\"sum\": 1232.8740052719259, \"count\": 42, \"avg\": 29.3541429826649, \"start\": \"2000-01-01\", \"end\": \"2000-02-11\"}}{\"partition\": 4, \"offset\": 205, \"A\": {\"sum\": 1333.8310093084442, \"count\": 41, \"avg\": 32.53246364166937, \"start\": \"2000-01-01\", \"end\": \"2000-02-10\"}, \"B\": {\"sum\": 927.3903420450616, \"count\": 41, \"avg\": 22.619276635245406, \"start\": \"2000-01-01\", \"end\": \"2000-02-10\"}, \"C\": {\"sum\": 1402.4445258822607, \"count\": 41, \"avg\": 34.2059640459088, \"start\": \"2000-01-01\", \"end\": \"2000-02-10\"}, \"K\": {\"sum\": 730.2510817696682, \"count\": 41, \"avg\": 17.811001994382153, \"start\": \"2000-01-01\", \"end\": \"2000-02-10\"}, \"L\": {\"sum\": 730.1094697920477, \"count\": 41, \"avg\": 17.80754804370848, \"start\": \"2000-01-01\", \"end\": \"2000-02-10\"}}{\"partition\": 5, \"offset\": 41, \"H\": {\"sum\": 941.9838548334851, \"count\": 41, \"avg\": 22.97521597154842, \"start\": \"2000-01-01\", \"end\": \"2000-02-10\"}}"
     ]
    }
   ],
   "source": [
    "! cat partition*.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
